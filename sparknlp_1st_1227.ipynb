{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.3.4\n",
      "Apache Spark version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    " \n",
    "spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    " \n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    " \n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8635403"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = spark.read.csv('_data/dataset_review.csv',inferSchema=True,header=True)\n",
    "reviews = reviews.select(['business_id', 'text', 'stars'])\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "reviews = reviews.withColumn('stars', col('stars').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1262800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_stars = reviews.filter(reviews.stars == 5.0)\n",
    "one_stars = reviews.filter(reviews.stars == 1.0)\n",
    "num_one_stars = one_stars.count()\n",
    "five_stars = five_stars.limit(num_one_stars)\n",
    "one_or_five_stars = five_stars.union(one_stars)\n",
    "print(five_stars.count())\n",
    "num_one_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol = 'stars', outputCol = 'categoryIndex')\n",
    "indexed = indexer.fit(one_or_five_stars).transform(one_or_five_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631353\n"
     ]
    }
   ],
   "source": [
    "x=0.25\n",
    "subset_df, large_df = indexed.randomSplit([x, 1 - x])\n",
    "subset_count = subset_df.count()\n",
    "print(subset_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Build NLP preprocessing pipeline\n",
    "from sparknlp.base import DocumentAssembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol('text') \\\n",
    ".setOutputCol('document')\n",
    "from sparknlp.annotator import Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    ".setInputCols(['document']) \\\n",
    ".setOutputCol('tokenized') \\\n",
    ".setContextChars(['(', ')']) \\\n",
    ".setSplitChars(['-'])\n",
    "from sparknlp.annotator import Normalizer\n",
    "normalizer = Normalizer() \\\n",
    ".setInputCols(['tokenized']) \\\n",
    ".setOutputCol('normalized') \\\n",
    ".setLowercase(True) \\\n",
    ".setCleanupPatterns(['[^A-Za-z]'])\n",
    "from sparknlp.annotator import LemmatizerModel\n",
    "lemmatizer = LemmatizerModel \\\n",
    ".pretrained() \\\n",
    ".setInputCols(['normalized']) \\\n",
    ".setOutputCol('lemmatized')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    ".setInputCols(['lemmatized']) \\\n",
    ".setOutputCol('unigrams') \\\n",
    ".setStopWords(nltk_stopwords)\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['unigrams']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(2) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "from sparknlp.base import Finisher\n",
    "finisher = Finisher() \\\n",
    ".setInputCols(['unigrams', 'ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline() \\\n",
    ".setStages([document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            lemmatizer,\n",
    "            stopwords_cleaner,\n",
    "            ngrammer,\n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = pipeline.fit(subset_df).transform(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='--0DF12EMHYI8XIgoFha6A', text=\"Being from Chicago originally and moving this year  we were in the market for a good mechanic. A good  honest mechanic. Joe himself answers the phone when we call and is the epitome of southern hospitality. We have had our car in his shop twice since October and his work is always excellent and his prices are honest. He won't try and convince you to pay for bogus parts you don't need or fake repairs. A friend recommended him for his honesty and affordability. After dropping the car off one time  he offered to drive us home so we wouldn't have to walk 2 miles-AND- my husband is somewhat handy with cars and Joe offered to let him borrows the special wrench he would need to install it himself-FOR FREE- and Joe could order the part for him (O2 sensor) and only charge us for that and not labor! Joe even said we could do it at his shop!  We are greatful for Joe and would recommend him to anyone looking for a good mechanic :)\", stars='5.0', categoryIndex=1.0, finished_unigrams=['chicago', 'originally', 'move', 'year', 'market', 'good', 'mechanic', 'good', 'honest', 'mechanic', 'joe', 'answer', 'phone', 'call', 'epitome', 'southern', 'hospitality', 'car', 'shop', 'twice', 'since', 'october', 'work', 'always', 'excellent', 'price', 'honest', 'wont', 'try', 'convince', 'pay', 'bogus', 'part', 'dont', 'need', 'fake', 'repair', 'friend', 'recommend', 'honesty', 'affordability', 'drop', 'car', 'one', 'time', 'offer', 'drive', 'home', 'wouldnt', 'walk', 'mile', 'husband', 'somewhat', 'handy', 'car', 'joe', 'offer', 'let', 'borrow', 'special', 'wrench', 'would', 'need', 'install', 'free', 'joe', 'could', 'order', 'part', 'sensor', 'charge', 'labor', 'joe', 'even', 'say', 'could', 'shop', 'greatful', 'joe', 'would', 'recommend', 'anyone', 'look', 'good', 'mechanic'], finished_ngrams=['chicago', 'originally', 'move', 'year', 'market', 'good', 'mechanic', 'good', 'honest', 'mechanic', 'joe', 'answer', 'phone', 'call', 'epitome', 'southern', 'hospitality', 'car', 'shop', 'twice', 'since', 'october', 'work', 'always', 'excellent', 'price', 'honest', 'wont', 'try', 'convince', 'pay', 'bogus', 'part', 'dont', 'need', 'fake', 'repair', 'friend', 'recommend', 'honesty', 'affordability', 'drop', 'car', 'one', 'time', 'offer', 'drive', 'home', 'wouldnt', 'walk', 'mile', 'husband', 'somewhat', 'handy', 'car', 'joe', 'offer', 'let', 'borrow', 'special', 'wrench', 'would', 'need', 'install', 'free', 'joe', 'could', 'order', 'part', 'sensor', 'charge', 'labor', 'joe', 'even', 'say', 'could', 'shop', 'greatful', 'joe', 'would', 'recommend', 'anyone', 'look', 'good', 'mechanic', 'chicago_originally', 'originally_move', 'move_year', 'year_market', 'market_good', 'good_mechanic', 'mechanic_good', 'good_honest', 'honest_mechanic', 'mechanic_joe', 'joe_answer', 'answer_phone', 'phone_call', 'call_epitome', 'epitome_southern', 'southern_hospitality', 'hospitality_car', 'car_shop', 'shop_twice', 'twice_since', 'since_october', 'october_work', 'work_always', 'always_excellent', 'excellent_price', 'price_honest', 'honest_wont', 'wont_try', 'try_convince', 'convince_pay', 'pay_bogus', 'bogus_part', 'part_dont', 'dont_need', 'need_fake', 'fake_repair', 'repair_friend', 'friend_recommend', 'recommend_honesty', 'honesty_affordability', 'affordability_drop', 'drop_car', 'car_one', 'one_time', 'time_offer', 'offer_drive', 'drive_home', 'home_wouldnt', 'wouldnt_walk', 'walk_mile', 'mile_husband', 'husband_somewhat', 'somewhat_handy', 'handy_car', 'car_joe', 'joe_offer', 'offer_let', 'let_borrow', 'borrow_special', 'special_wrench', 'wrench_would', 'would_need', 'need_install', 'install_free', 'free_joe', 'joe_could', 'could_order', 'order_part', 'part_sensor', 'sensor_charge', 'charge_labor', 'labor_joe', 'joe_even', 'even_say', 'say_could', 'could_shop', 'shop_greatful', 'greatful_joe', 'joe_would', 'would_recommend', 'recommend_anyone', 'anyone_look', 'look_good', 'good_mechanic'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_reviews.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = processed_reviews.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505082.4\n"
     ]
    }
   ],
   "source": [
    "trainingData_count = subset_count * 0.8\n",
    "print(trainingData_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfizer = CountVectorizer(inputCol = 'finished_ngrams', outputCol = 'tf_features', \n",
    "                         minDF = 0.01, maxDF = 0.1, vocabSize = int(trainingData_count / 2))\n",
    "\n",
    "tf_model = tfizer.fit(trainingData)\n",
    "tf_result_training = tf_model.transform(trainingData)\n",
    "tf_result_test = tf_model.transform(testData)\n",
    "\n",
    "idfizer = IDF(inputCol = 'tf_features', outputCol = 'tfidf_features')\n",
    "\n",
    "idf_model = idfizer.fit(tf_result_training)\n",
    "tfidf_result_training = idf_model.transform(tf_result_training)\n",
    "tfidf_result_test = idf_model.transform(tf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.save(\"_data/tfModel.model\")\n",
    "idf_model.save(\"_data/idfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         business_id|                text|stars|categoryIndex|   finished_unigrams|     finished_ngrams|         tf_features|      tfidf_features|\n",
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|--0DF12EMHYI8XIgo...|Being from Chicag...|  5.0|          1.0|[chicago, origina...|[chicago, origina...|(826,[0,1,8,14,26...|(826,[0,1,8,14,26...|\n",
      "|--0r8K_AQ4FZfLsX3...|I am really happy...|  5.0|          1.0|[really, happy, b...|[really, happy, b...|(826,[32,84,93,98...|(826,[32,84,93,98...|\n",
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         business_id|                text|stars|categoryIndex|   finished_unigrams|     finished_ngrams|         tf_features|      tfidf_features|\n",
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|--164t1nclzzmca7e...|I was introduced ...|  5.0|          1.0|[introduce, hungr...|[introduce, hungr...|(826,[5,14,26,27,...|(826,[5,14,26,27,...|\n",
      "|--164t1nclzzmca7e...|I'm reviewing: Th...|  5.0|          1.0|[im, review, bbq,...|[im, review, bbq,...|(826,[5,24,43,92,...|(826,[5,24,43,92,...|\n",
      "+--------------------+--------------------+-----+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_result_training.show(2))\n",
    "print(tfidf_result_test.show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer並CountVectorizerModel旨在幫助將文本文檔集合轉換為標記計數向量。\n",
    "# 當先驗字典不可用時，CountVectorizer可用作Estimator提取詞彙表，並生成CountVectorizerModel. \n",
    "# 該模型通過詞彙表為文檔生成稀疏表示，然後可以將其傳遞給其他算法，如 LDA。\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "tf_model = CountVectorizerModel.load(\"file:/home/john/_data/tfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDFModel\n",
    "idf_model = IDFModel.load(\"file:/home/john/_data/idfModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826\n"
     ]
    }
   ],
   "source": [
    "# Print vocablary length (i.e. # of columns)\n",
    "print(len(tf_model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'pay',\n",
       " 'two',\n",
       " 'drink',\n",
       " 'another',\n",
       " 'review',\n",
       " 'table',\n",
       " 'still',\n",
       " 'walk',\n",
       " 'right',\n",
       " 'amazing',\n",
       " 'long',\n",
       " 'chicken',\n",
       " 'check',\n",
       " 'friend',\n",
       " 'around',\n",
       " 'manager',\n",
       " 'definitely',\n",
       " 'late',\n",
       " 'taste',\n",
       " 'every',\n",
       " 'last',\n",
       " 'sure',\n",
       " 'store',\n",
       " 'delicious',\n",
       " 'location',\n",
       " 'since',\n",
       " 'everything',\n",
       " 'next',\n",
       " 'cant',\n",
       " 'little',\n",
       " 'night',\n",
       " 'business',\n",
       " 'week',\n",
       " 'guy',\n",
       " 'visit',\n",
       " 'clean',\n",
       " 'sit',\n",
       " 'care',\n",
       " 'menu',\n",
       " 'room',\n",
       " 'help',\n",
       " 'bring',\n",
       " 'lot',\n",
       " 'customer_service',\n",
       " 'home',\n",
       " 'money',\n",
       " 'start',\n",
       " 'bar',\n",
       " 'many',\n",
       " 'charge',\n",
       " 'put',\n",
       " 'end',\n",
       " 'star',\n",
       " 'pizza',\n",
       " 'something',\n",
       " 'away',\n",
       " 'keep',\n",
       " 'wasnt',\n",
       " 'seem',\n",
       " 'close',\n",
       " 'area',\n",
       " 'fry',\n",
       " 'move',\n",
       " 'phone',\n",
       " 'buy',\n",
       " 'show',\n",
       " 'let',\n",
       " 'company',\n",
       " 'return',\n",
       " 'open',\n",
       " 'meal',\n",
       " 'go_back',\n",
       " 'month',\n",
       " 'rude',\n",
       " 'owner',\n",
       " 'come_back',\n",
       " 'small',\n",
       " 'server',\n",
       " 'park',\n",
       " 'seat',\n",
       " 'offer',\n",
       " 'shop',\n",
       " 'someone',\n",
       " 'super',\n",
       " 'anything',\n",
       " 'line',\n",
       " 'nothing',\n",
       " 'fresh',\n",
       " 'hair',\n",
       " 'serve',\n",
       " 'old',\n",
       " 'sauce',\n",
       " 'pick',\n",
       " 'job',\n",
       " 'pretty',\n",
       " 'top',\n",
       " 'decide',\n",
       " 'happy',\n",
       " 'stay',\n",
       " 'worth',\n",
       " 'drive',\n",
       " 'else',\n",
       " 'person',\n",
       " 'cheese',\n",
       " 'without',\n",
       " 'front',\n",
       " 'finally',\n",
       " 'different',\n",
       " 'stop',\n",
       " 'enough',\n",
       " 'dish',\n",
       " 'side',\n",
       " 'receive',\n",
       " 'big',\n",
       " 'run',\n",
       " 'appointment',\n",
       " 'change',\n",
       " 'point',\n",
       " 'live',\n",
       " 'quality',\n",
       " 'thank',\n",
       " 'talk',\n",
       " 'favorite',\n",
       " 'arrive',\n",
       " 'couldnt',\n",
       " 'though',\n",
       " 'actually',\n",
       " 'youre',\n",
       " 'door',\n",
       " 'issue',\n",
       " 'problem',\n",
       " 'family',\n",
       " 'dinner',\n",
       " 'awesome',\n",
       " 'salad',\n",
       " 'coffee',\n",
       " 'highly',\n",
       " 'enjoy',\n",
       " 'employee',\n",
       " 'full',\n",
       " 'today',\n",
       " 'horrible',\n",
       " 'water',\n",
       " 'burger',\n",
       " 'everyone',\n",
       " 'free',\n",
       " 'far',\n",
       " 'cook',\n",
       " 'half',\n",
       " 'absolutely',\n",
       " 'wont',\n",
       " 'flavor',\n",
       " 'wrong',\n",
       " 'dog',\n",
       " 'terrible',\n",
       " 'house',\n",
       " 'kind',\n",
       " 'hand',\n",
       " 'fix',\n",
       " 'nail',\n",
       " 'speak',\n",
       " 'lunch',\n",
       " 'part',\n",
       " 'deal',\n",
       " 'sandwich',\n",
       " 'item',\n",
       " 'excellent',\n",
       " 'spend',\n",
       " 'anyone',\n",
       " 'pm',\n",
       " 'hot',\n",
       " 'name',\n",
       " 'send',\n",
       " 'waitress',\n",
       " 'option',\n",
       " 'beer',\n",
       " 'extremely',\n",
       " 'cut',\n",
       " 'expect',\n",
       " 'spot',\n",
       " 'perfect',\n",
       " 'happen',\n",
       " 'almost',\n",
       " 'high',\n",
       " 'disappointed',\n",
       " 'least',\n",
       " 'able',\n",
       " 'treat',\n",
       " 'several',\n",
       " 'hear',\n",
       " 'bit',\n",
       " 'husband',\n",
       " 'second',\n",
       " 'hard',\n",
       " 'ill',\n",
       " 'id',\n",
       " 'office',\n",
       " 'card',\n",
       " 'sweet',\n",
       " 'highly_recommend',\n",
       " 'whole',\n",
       " 'however',\n",
       " 'three',\n",
       " 'meat',\n",
       " 'doesnt',\n",
       " 'turn',\n",
       " 'couple',\n",
       " 'busy',\n",
       " 'kid',\n",
       " 'special',\n",
       " 'first_time',\n",
       " 'less',\n",
       " 'please',\n",
       " 'already',\n",
       " 'cold',\n",
       " 'bill',\n",
       " 'instead',\n",
       " 'explain',\n",
       " 'roll',\n",
       " 'add',\n",
       " 'purchase',\n",
       " 'austin',\n",
       " 'min',\n",
       " 'probably',\n",
       " 'plate',\n",
       " 'large',\n",
       " 'maybe',\n",
       " 'outside',\n",
       " 'inside',\n",
       " 'helpful',\n",
       " 'book',\n",
       " 'lady',\n",
       " 'wouldnt',\n",
       " 'hotel',\n",
       " 'write',\n",
       " 'extra',\n",
       " 'taco',\n",
       " 'ago',\n",
       " 'sign',\n",
       " 'understand',\n",
       " 'rice',\n",
       " 'cost',\n",
       " 'wonderful',\n",
       " 'break',\n",
       " 'completely',\n",
       " 'set',\n",
       " 'fast',\n",
       " 'atmosphere',\n",
       " 'life',\n",
       " 'ice',\n",
       " 'answer',\n",
       " 'bread',\n",
       " 'party',\n",
       " 'must',\n",
       " 'mean',\n",
       " 'stand',\n",
       " 'ok',\n",
       " 'professional',\n",
       " 'town',\n",
       " 'woman',\n",
       " 'waste',\n",
       " 'hope',\n",
       " 'wife',\n",
       " 'waiter',\n",
       " 'read',\n",
       " 'feel_like',\n",
       " 'question',\n",
       " 'breakfast',\n",
       " 'huge',\n",
       " 'dr',\n",
       " 'state',\n",
       " 'light',\n",
       " 'tip',\n",
       " 'reason',\n",
       " 'selection',\n",
       " 'yet',\n",
       " 'girl',\n",
       " 'fill',\n",
       " 'fun',\n",
       " 'man',\n",
       " 'make_sure',\n",
       " 'oh',\n",
       " 'wish',\n",
       " 'notice',\n",
       " 'entire',\n",
       " 'poor',\n",
       " 'quick',\n",
       " 'fine',\n",
       " 'reservation',\n",
       " 'finish',\n",
       " 'may',\n",
       " 'cream',\n",
       " 'fantastic',\n",
       " 'piece',\n",
       " 'include',\n",
       " 'sell',\n",
       " 'usually',\n",
       " 'wine',\n",
       " 'dry',\n",
       " 'especially',\n",
       " 'mention',\n",
       " 'morning',\n",
       " 'ive_ever',\n",
       " 'early',\n",
       " 'stuff',\n",
       " 'delivery',\n",
       " 'course',\n",
       " 'local',\n",
       " 'sushi',\n",
       " 'ready',\n",
       " 'number',\n",
       " 'soup',\n",
       " 'provide',\n",
       " 'easy',\n",
       " 'either',\n",
       " 'steak',\n",
       " 'boston',\n",
       " 'yelp',\n",
       " 'miss',\n",
       " 'management',\n",
       " 'group',\n",
       " 'size',\n",
       " 'believe',\n",
       " 'street',\n",
       " 'theyre',\n",
       " 'yes',\n",
       " 'cheap',\n",
       " 'fact',\n",
       " 'salon',\n",
       " 'watch',\n",
       " 'past',\n",
       " 'quite',\n",
       " 'counter',\n",
       " 'literally',\n",
       " 'choose',\n",
       " 'portion',\n",
       " 'contact',\n",
       " 'meet',\n",
       " 'egg',\n",
       " 'short',\n",
       " 'deliver',\n",
       " 'schedule',\n",
       " 'tea',\n",
       " 'isnt',\n",
       " 'credit',\n",
       " 'plan',\n",
       " 'real',\n",
       " 'floor',\n",
       " 'repair',\n",
       " 'follow',\n",
       " 'due',\n",
       " 'glass',\n",
       " 'refund',\n",
       " 'dress',\n",
       " 'amount',\n",
       " 'online',\n",
       " 'fish',\n",
       " 'guess',\n",
       " 'tasty',\n",
       " 'look_like',\n",
       " 'bartender',\n",
       " 'cool',\n",
       " 'kitchen',\n",
       " 'awful',\n",
       " 'beef',\n",
       " 'pork',\n",
       " 'request',\n",
       " 'list',\n",
       " 'even_though',\n",
       " 'cake',\n",
       " 'portland',\n",
       " 'twice',\n",
       " 'surprise',\n",
       " 'forget',\n",
       " 'dessert',\n",
       " 'soon',\n",
       " 'hold',\n",
       " 'might',\n",
       " 'trip',\n",
       " 'regular',\n",
       " 'head',\n",
       " 'mind',\n",
       " 'lose',\n",
       " 'attitude',\n",
       " 'behind',\n",
       " 'beautiful',\n",
       " 'process',\n",
       " 'product',\n",
       " 'immediately',\n",
       " 'quickly',\n",
       " 'choice',\n",
       " 'rate',\n",
       " 'spicy',\n",
       " 'save',\n",
       " 'eye',\n",
       " 'desk',\n",
       " 'dollar',\n",
       " 'appetizer',\n",
       " 'shrimp',\n",
       " 'dont_know',\n",
       " 'low',\n",
       " 'dining',\n",
       " 'plus',\n",
       " 'overall',\n",
       " 'lack',\n",
       " 'class',\n",
       " 'bowl',\n",
       " 'drop',\n",
       " 'cover',\n",
       " 'play',\n",
       " 'within',\n",
       " 'massage',\n",
       " 'sorry',\n",
       " 'potato',\n",
       " 'oil',\n",
       " 'refuse',\n",
       " 'replace',\n",
       " 'saturday',\n",
       " 'available',\n",
       " 'every_time',\n",
       " 'avoid',\n",
       " 'cancel',\n",
       " 'dirty',\n",
       " 'apartment',\n",
       " 'window',\n",
       " 'doctor',\n",
       " 'call_back',\n",
       " 'box',\n",
       " 'chip',\n",
       " 'pass',\n",
       " 'pull',\n",
       " 'fee',\n",
       " 'date',\n",
       " 'totally',\n",
       " 'warm',\n",
       " 'slow',\n",
       " 'bite',\n",
       " 'style',\n",
       " 'continue',\n",
       " 'greet',\n",
       " 'expensive',\n",
       " 'weekend',\n",
       " 'music',\n",
       " 'insurance',\n",
       " 'clearly',\n",
       " 'begin',\n",
       " 'seriously',\n",
       " 'type',\n",
       " 'share',\n",
       " 'suppose',\n",
       " 'complete',\n",
       " 'wing',\n",
       " 'build',\n",
       " 'interest',\n",
       " 'face',\n",
       " 'space',\n",
       " 'chocolate',\n",
       " 'welcome',\n",
       " 'email',\n",
       " 'green',\n",
       " 'mistake',\n",
       " 'red',\n",
       " 'smell',\n",
       " 'patient',\n",
       " 'post',\n",
       " 'consider',\n",
       " 'picture',\n",
       " 'base',\n",
       " 'color',\n",
       " 'daughter',\n",
       " 'etc',\n",
       " 'word',\n",
       " 'throw',\n",
       " 'rent',\n",
       " 'cannot',\n",
       " 'hang',\n",
       " 'never_go',\n",
       " 'bag',\n",
       " 'realize',\n",
       " 'birthday',\n",
       " 'allow',\n",
       " 'simple',\n",
       " 'city',\n",
       " 'recently',\n",
       " 'rather',\n",
       " 'inform',\n",
       " 'vehicle',\n",
       " 'love_place',\n",
       " 'truck',\n",
       " 'across',\n",
       " 'perfectly',\n",
       " 'taste_like',\n",
       " 'multiple',\n",
       " 'complain',\n",
       " 'idea',\n",
       " 'white',\n",
       " 'empty',\n",
       " 'often',\n",
       " 'noodle',\n",
       " 'reasonable',\n",
       " 'would_recommend',\n",
       " 'sunday',\n",
       " 'situation',\n",
       " 'remember',\n",
       " 'suggest',\n",
       " 'truly',\n",
       " 'black',\n",
       " 'stick',\n",
       " 'total',\n",
       " 'response',\n",
       " 'okay',\n",
       " 'simply',\n",
       " 'wow',\n",
       " 'child',\n",
       " 'attentive',\n",
       " 'figure',\n",
       " 'havent',\n",
       " 'sales',\n",
       " 'prepare',\n",
       " 'son',\n",
       " 'fall',\n",
       " 'case',\n",
       " 'clear',\n",
       " 'team',\n",
       " 'grill',\n",
       " 'didnt_even',\n",
       " 'wash',\n",
       " 'honestly',\n",
       " 'rest',\n",
       " 'game',\n",
       " 'five',\n",
       " 'decent',\n",
       " 'beyond',\n",
       " 'glad',\n",
       " 'upon',\n",
       " 'lie',\n",
       " 'say_would',\n",
       " 'near',\n",
       " 'th',\n",
       " 'together',\n",
       " 'friday',\n",
       " 'world',\n",
       " 'establishment',\n",
       " 'comfortable',\n",
       " 'driver',\n",
       " 'forward',\n",
       " 'single',\n",
       " 'although',\n",
       " 'four',\n",
       " 'chef',\n",
       " 'quote',\n",
       " 'exactly',\n",
       " 'mess',\n",
       " 'guest',\n",
       " 'cause',\n",
       " 'boyfriend',\n",
       " 'wait_minute',\n",
       " 'bean',\n",
       " 'along',\n",
       " 'unfortunately',\n",
       " 'zero',\n",
       " 'thai',\n",
       " 'foot',\n",
       " 'bother',\n",
       " 'bathroom',\n",
       " 'wall',\n",
       " 'pack',\n",
       " 'impressed',\n",
       " 'member',\n",
       " 'detail',\n",
       " 'note',\n",
       " 'wear',\n",
       " 'werent',\n",
       " 'bbq',\n",
       " 'young',\n",
       " 'great_service',\n",
       " 'atlanta',\n",
       " 'ice_cream',\n",
       " 'bacon',\n",
       " 'rush',\n",
       " 'trust',\n",
       " 'agree',\n",
       " 'system',\n",
       " 'make_feel',\n",
       " 'youll',\n",
       " 'cup',\n",
       " 'basically',\n",
       " 'slice',\n",
       " 'chance',\n",
       " 'attention',\n",
       " 'unprofessional',\n",
       " 'mix',\n",
       " 'fan',\n",
       " 'next_day',\n",
       " 'appreciate',\n",
       " 'plenty',\n",
       " 'season',\n",
       " 'next_time',\n",
       " 'bed',\n",
       " 'matter',\n",
       " 'crowd',\n",
       " 'knowledgeable',\n",
       " 'grab',\n",
       " 'anyway',\n",
       " 'brunch',\n",
       " 'anywhere',\n",
       " 'incredibly',\n",
       " 'become',\n",
       " 'somewhere',\n",
       " 'claim',\n",
       " 'level',\n",
       " 'complaint',\n",
       " 'time_go',\n",
       " 'apparently',\n",
       " 'touch',\n",
       " 'cash',\n",
       " 'apologize',\n",
       " 'learn',\n",
       " 'hit',\n",
       " 'worker',\n",
       " 'handle',\n",
       " 'front_desk',\n",
       " 'take_care',\n",
       " 'fit',\n",
       " 'website',\n",
       " 'listen',\n",
       " 'update',\n",
       " 'suck',\n",
       " 'step',\n",
       " 'really_good',\n",
       " 'bad_experience',\n",
       " 'unless',\n",
       " 'result',\n",
       " 'onion',\n",
       " 'accommodate',\n",
       " 'great_food',\n",
       " 'negative',\n",
       " 'information',\n",
       " 'damage',\n",
       " 'tell_would',\n",
       " 'event',\n",
       " 'waste_time',\n",
       " 'ingredient',\n",
       " 'elsewhere',\n",
       " 'chinese',\n",
       " 'pain',\n",
       " 'sound',\n",
       " 'burn',\n",
       " 'possible',\n",
       " 'excited',\n",
       " 'treatment',\n",
       " 'obviously',\n",
       " 'chair',\n",
       " 'relax',\n",
       " 'weve',\n",
       " 'ignore',\n",
       " 'authentic',\n",
       " 'minute_late',\n",
       " 'disgusting',\n",
       " 'hate',\n",
       " 'one_good',\n",
       " 'recommend_place',\n",
       " 'hostess',\n",
       " 'pleasant',\n",
       " 'support',\n",
       " 'last_time',\n",
       " 'take_order',\n",
       " 'great_place',\n",
       " 'cocktail',\n",
       " 'pasta',\n",
       " 'mouth',\n",
       " 'credit_card',\n",
       " 'rib',\n",
       " 'hes',\n",
       " 'sad',\n",
       " 'remove',\n",
       " 'bottle',\n",
       " 'let_know',\n",
       " 'mexican',\n",
       " 'story',\n",
       " 'tonight',\n",
       " 'incredible',\n",
       " 'ahead',\n",
       " 'year_old',\n",
       " 'ridiculous',\n",
       " 'variety',\n",
       " 'machine',\n",
       " 'smile',\n",
       " 'main',\n",
       " 'food_good',\n",
       " 'correct',\n",
       " 'attempt',\n",
       " 'school',\n",
       " 'im_sure',\n",
       " 'neighborhood',\n",
       " 'wrap',\n",
       " 'non',\n",
       " 'smoke',\n",
       " 'tomato',\n",
       " 'orlando',\n",
       " 'promise',\n",
       " 'year_ago',\n",
       " 'place_go',\n",
       " 'act',\n",
       " 'bland',\n",
       " 'message',\n",
       " 'center',\n",
       " 'blow',\n",
       " 'respond',\n",
       " 'client',\n",
       " 'take_time',\n",
       " 'ive_never',\n",
       " 'arent',\n",
       " 'disappointing',\n",
       " 'mile',\n",
       " 'true',\n",
       " 'proceed',\n",
       " 'honest',\n",
       " 'whatever',\n",
       " 'value',\n",
       " 'future',\n",
       " 'barely',\n",
       " 'sometimes',\n",
       " 'go_get',\n",
       " 'long_time',\n",
       " 'seem_like',\n",
       " 'conversation',\n",
       " 'could_give',\n",
       " 'joke',\n",
       " 'despite',\n",
       " 'freeze',\n",
       " 'per',\n",
       " 'cant_wait',\n",
       " 'loud',\n",
       " 'shes',\n",
       " 'food_great',\n",
       " 'die',\n",
       " 'get_food',\n",
       " 'cute',\n",
       " 'travel',\n",
       " 'patio',\n",
       " 'excuse',\n",
       " 'good_food',\n",
       " 'middle',\n",
       " 'staff_friendly',\n",
       " 'view',\n",
       " 'decor',\n",
       " 'hungry',\n",
       " 'overpriced',\n",
       " 'afternoon',\n",
       " 'hire',\n",
       " 'tender',\n",
       " 'french',\n",
       " 'address',\n",
       " 'practice',\n",
       " 'personal',\n",
       " 'discount',\n",
       " 'fair',\n",
       " 'standard',\n",
       " 'upset',\n",
       " 'crispy',\n",
       " 'yummy',\n",
       " 'confirm',\n",
       " 'never_come',\n",
       " 'none',\n",
       " 'unique',\n",
       " 'didnt_want',\n",
       " 'heat',\n",
       " 'good_thing',\n",
       " 'last_night',\n",
       " 'previous',\n",
       " 'brand',\n",
       " 'st',\n",
       " 'comment',\n",
       " 'flavorful',\n",
       " 'downtown',\n",
       " 'sort',\n",
       " 'fail',\n",
       " 'service_great',\n",
       " 'ton',\n",
       " 'time_get',\n",
       " 'deserve',\n",
       " 'recommendation',\n",
       " 'positive',\n",
       " 'original',\n",
       " 'much_well',\n",
       " 'lovely',\n",
       " 'additional',\n",
       " 'locate',\n",
       " 'yesterday',\n",
       " 'manage',\n",
       " 'prior',\n",
       " 'folk',\n",
       " 'warn',\n",
       " 'would_never',\n",
       " 'wonder',\n",
       " 'general',\n",
       " 'bad_service',\n",
       " 'double',\n",
       " 'great_experience',\n",
       " 'stay_away',\n",
       " 'rip',\n",
       " 'get_back',\n",
       " 'tiny',\n",
       " 'hadnt',\n",
       " 'several_time',\n",
       " 'look_forward',\n",
       " 'vibe',\n",
       " 'try_get',\n",
       " 'enter',\n",
       " 'take_minute',\n",
       " 'write_review',\n",
       " 'straight',\n",
       " 'one_star',\n",
       " 'moment',\n",
       " 'mediocre',\n",
       " 'except',\n",
       " 'sense',\n",
       " 'outstanding',\n",
       " 'crazy',\n",
       " 'anymore',\n",
       " 'absolute',\n",
       " 'forever',\n",
       " 'food_service',\n",
       " 'easily']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory topic modeling\n",
    "# 潛在狄利克雷分配 (LDA)\n",
    "# LDA被實現為Estimator同時支持EMLDAOptimizer和OnlineLDAOptimizer，並生成 aLDAModel作為基本模型。如果需要，專家用戶可以將LDAModel生成的EMLDAOptimizer轉換為生成的 DistributedLDAModel\n",
    "from pyspark.ml.clustering import LDA\n",
    "num_topics = 10\n",
    "max_iter = 10\n",
    "lda = LDA(k = num_topics, \n",
    "          maxIter = max_iter, \n",
    "          featuresCol = 'tfidf_features')\n",
    "ldaModel = lda.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "vocab = tf_model.vocabulary\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------+\n",
      "|topic|                                                                         topicWords|\n",
      "+-----+-----------------------------------------------------------------------------------+\n",
      "|    0|       [car, rude, manager, dog, store, employee, customer_service, guy, buy, walk]|\n",
      "|    1|[pizza, wing, tea, server, great_service, every, drink, cup, every_time, excellent]|\n",
      "|    2|            [chicken, delicious, fry, taste, sauce, dish, salad, fresh, menu, rice]|\n",
      "|    3|  [dr, appointment, doctor, cream, ice, care, ice_cream, patient, daughter, office]|\n",
      "|    4|               [massage, park, bar, beer, house, local, selection, space, fun, lot]|\n",
      "|    5|             [nail, drink, bartender, bar, burger, game, play, crowd, night, close]|\n",
      "|    6|              [room, hotel, delivery, stay, deliver, coffee, bed, book, desk, thai]|\n",
      "|    7|         [hair, cut, store, sandwich, dress, salon, burger, product, color, cheese]|\n",
      "|    8|                  [company, car, move, month, pay, charge, phone, week, issue, fix]|\n",
      "|    9|            [table, waiter, card, server, waitress, sit, meal, credit, drink, cold]|\n",
      "+-----+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 取10個字詞\n",
    "num_top_words = 10\n",
    "topics = ldaModel \\\n",
    ".describeTopics(num_top_words) \\\n",
    ".withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logistic regression with ridge\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'tfidf_features', labelCol = 'categoryIndex', \n",
    "                        family = 'binomial', elasticNetParam = 0, regParam = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggregationDepth': 2,\n",
       " 'elasticNetParam': 0.0,\n",
       " 'family': 'binomial',\n",
       " 'featuresCol': 'tfidf_features',\n",
       " 'fitIntercept': True,\n",
       " 'labelCol': 'categoryIndex',\n",
       " 'maxBlockSizeInMB': 0.0,\n",
       " 'maxIter': 100,\n",
       " 'predictionCol': 'prediction',\n",
       " 'probabilityCol': 'probability',\n",
       " 'rawPredictionCol': 'rawPrediction',\n",
       " 'regParam': 0.1,\n",
       " 'standardization': True,\n",
       " 'threshold': 0.5,\n",
       " 'tol': 1e-06}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all parameters\n",
    "{param[0].name: param[1] for param in lr.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LR model\n",
    "lrModel = lr.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"_data/lrModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 邏輯回歸是一種流行的預測分類響應的方法。它是預測結果概率的廣義線性模型的特例。在spark.ml邏輯回歸中，可以使用二項邏輯回歸來預測二元結果\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "lrModel = LogisticRegressionModel.load(\"file:/home/john/_data/lrModel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions_training = lrModel.transform(tfidf_result_training)\n",
    "lrPredictions_test = lrModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML 中的一項重要任務是模型選擇，或使用數據為給定任務找到最佳模型或參數。這也稱為調諧。可以針對單個EstimatorsLogisticRegression或Pipeline包括多個算法、特徵化和其他步驟的整個s 進行調整。用戶可以Pipeline一次調整整體，而不是Pipeline單獨調整每個元素\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'categoryIndex', predictionCol = 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.ml向量和矩陣類型。用於轉換實用工具DataFrame從列spark.mllib.linalg到spark.ml.linalg類型\n",
    "acc_training_lr = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "acc_test_lr = evaluator.evaluate(lrPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# f1 = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator.evaluate(lrPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9321971173731405\n",
      "Test accuracy: 0.9325505751685839\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy: ' + str(acc_training_lr))\n",
    "print('Test accuracy: ' + str(acc_test_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系數矩陣\n",
    "coef_matrix = lrModel.coefficientMatrix\n",
    "coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>816</th>\n",
       "      <th>817</th>\n",
       "      <th>818</th>\n",
       "      <th>819</th>\n",
       "      <th>820</th>\n",
       "      <th>821</th>\n",
       "      <th>822</th>\n",
       "      <th>823</th>\n",
       "      <th>824</th>\n",
       "      <th>825</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115201</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 826 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5    6         7    8    \\\n",
       "0  0.0  0.115201  0.000003  0.000008  0.000609  0.002514  0.0  0.000546  0.0   \n",
       "\n",
       "   9    ...  816  817  818       819  820  821  822  823  824       825  \n",
       "0  0.0  ...  0.0  0.0  0.0  0.000272  0.0  0.0  0.0  0.0  0.0  0.000008  \n",
       "\n",
       "[1 rows x 826 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(coef_list).T.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8d575f9782f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoef_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoef_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/john/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4103\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "coef_df = pd.DataFrame(coef_list).T.sort_values(0, ascending = True)\n",
    "for i in range(0, 20):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機森林 是決策樹的集合。隨機森林是用於分類和回歸的最成功的機器學習模型之一。它們結合了許多決策樹以降低過度擬合的風險。與決策樹一樣，隨機森林處理分類特徵，擴展到多類分類設置，不需要特徵縮放，並且能夠捕捉非線性和特徵交互。\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'tfidf_features', labelCol = 'categoryIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'cacheNodeIds': False,\n",
       " 'checkpointInterval': 10,\n",
       " 'featureSubsetStrategy': 'auto',\n",
       " 'featuresCol': 'tfidf_features',\n",
       " 'impurity': 'gini',\n",
       " 'labelCol': 'categoryIndex',\n",
       " 'leafCol': '',\n",
       " 'maxBins': 32,\n",
       " 'maxDepth': 5,\n",
       " 'maxMemoryInMB': 256,\n",
       " 'minInfoGain': 0.0,\n",
       " 'minInstancesPerNode': 1,\n",
       " 'minWeightFractionPerNode': 0.0,\n",
       " 'numTrees': 20,\n",
       " 'predictionCol': 'prediction',\n",
       " 'probabilityCol': 'probability',\n",
       " 'rawPredictionCol': 'rawPrediction',\n",
       " 'seed': 6889613275193631008,\n",
       " 'subsamplingRate': 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{param[0].name: param[1] for param in rf.extractParamMap().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(tfidf_result_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions_training = rfModel.transform(tfidf_result_training)\n",
    "rfPredictions_test = rfModel.transform(tfidf_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_training_rf = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"accuracy\"})\n",
    "acc_test_rf = evaluator.evaluate(rfPredictions_test, {evaluator.metricName: \"accuracy\"})\n",
    "# f1 = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"f1\"})\n",
    "# weightedPrecision = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# weightedRecall = evaluator.evaluate(rfPredictions_training, {evaluator.metricName: \"weightedRecall\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8124372901011868\n",
      "Test accuracy: 0.8121221737405792\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy: ' + str(acc_training_rf))\n",
    "print('Test accuracy: ' + str(acc_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix = rfModel.featureImportances\n",
    "coef_list = coef_matrix.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay\n",
      "horrible\n",
      "amazing\n",
      "definitely\n",
      "manager\n",
      "awesome\n",
      "delicious\n",
      "highly\n",
      "waste\n",
      "rude\n",
      "excellent\n",
      "money\n",
      "highly_recommend\n",
      "terrible\n",
      "poor\n",
      "fantastic\n",
      "someone\n",
      "charge\n",
      "perfect\n",
      "business\n",
      "customer_service\n",
      "nothing\n",
      "fresh\n",
      "late\n",
      "happen\n",
      "speak\n",
      "bill\n",
      "refund\n",
      "receive\n",
      "awful\n",
      "person\n",
      "wonderful\n",
      "cold\n",
      "ignore\n",
      "suppose\n",
      "dirty\n",
      "knowledgeable\n",
      "love_place\n",
      "company\n",
      "send\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "coef_df = pd.DataFrame(coef_list).sort_values(0, ascending = False)\n",
    "for i in range(0, 40):\n",
    "    print(tf_model.vocabulary[coef_df.index[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
