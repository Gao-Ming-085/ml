{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c92f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gaoming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gaoming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import sem\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "import numpy as np  # 書中遺漏此列程式碼\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c28ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id     object\n",
      "stars          float64\n",
      "text            object\n",
      "Tmp_number     float64\n",
      "type             int64\n",
      "dtype: object\n",
      "1    0.640913\n",
      "0    0.359087\n",
      "Name: type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sms_raw = pd.read_csv(\"./Comment.csv\")\n",
    "sms_raw.columns\n",
    "sms_raw = sms_raw.drop(\n",
    "    columns=['useful', 'funny', 'cool', 'Unnamed: 0', 'date'])\n",
    "\n",
    "\n",
    "def stars_level(star):\n",
    "    if star > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sms_raw['type'] = sms_raw['stars'].apply(stars_level)\n",
    "# type：垃圾或正常簡訊，text：簡訊文字內容\n",
    "print(sms_raw.dtypes)\n",
    "\n",
    "# type 次數分佈，ham 佔多數，但未過度不平衡\n",
    "print(sms_raw['type'].value_counts()/len(sms_raw['type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84cc54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrap', '-', 'falafel', 'with', 'greens', 'pickles']\n",
      "['I', 'ate', 'here', 'recently', 'on', 'the', 'recommendations', 'of', 'some', 'Muslim', 'brothers', 'at', 'the', 'Islamic', 'Society', 'of', 'Boston', '(', 'beautiful', 'masjid', ')', '.', 'They', 'said', 'it', 'was', 'halal', 'and', 'the', 'Arab', 'owners', 'confirmed', 'that', 'it', 'was', '.', 'This', 'is', 'a', 'nice', 'downtown', 'spot', 'across', 'the', 'street', 'from', 'the', 'park', 'and', 'right', 'next', 'to', 'the', 'Park', 'Street', 'subway', 'station.We', 'enjoyed', 'the', 'chicken', 'kabob', 'and', 'baked', 'haddock', 'dinners', 'which', 'both', 'came', 'with', 'rice', 'and', 'salad', '.', 'Very', 'enjoyable', 'hot', 'and', 'fresh', 'food', '.', 'When', 'I', 'went', 'to', 'buy', 'some', 'rice', 'pudding', 'for', 'dessert', 'the', 'owner', 'gave', 'it', 'to', 'us', 'for', 'free', '.', 'What', 'a', 'nice', 'touch', '.', 'InshaAllah', 'my', 'wife', 'and', 'I', 'will', 'return', 'soon', '.']\n"
     ]
    }
   ],
   "source": [
    "# Python 自然語言處理工具集(Natural Language ToolKit)\n",
    "#import nltk\n",
    "# 串列推導完成分詞\n",
    "token_list0 = [nltk.word_tokenize(txt) for txt in sms_raw['text']]\n",
    "print(token_list0[3][1:7])\n",
    "print(token_list0[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b477c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrap', '-', 'falafel', 'with', 'greens', 'pickles']\n",
      "['wrap', '-', 'falafel', 'greens', 'pickles', 'hummus']\n"
     ]
    }
   ],
   "source": [
    "# 串列推導完成轉小寫(Ibiza 變成ibiza)\n",
    "token_list1 = [[word.lower() for word in doc] for doc in token_list0]  \n",
    "# doc: 各則的各個字詞\n",
    "print(token_list1[3][1:7])\n",
    "\n",
    "\n",
    "# 移除停用詞\n",
    "#from nltk.corpus import stopwords\n",
    "# 179 個英語停用字詞\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 停用字 or 已被移除\n",
    "token_list2 = [[word for word in doc if word not in stop_words] for doc in token_list1]\n",
    "print(token_list2[3][1:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84361577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrap', 'falafel', 'greens', 'pickles', 'hummus', 'toasted']\n",
      "['wrap', 'falafel', 'greens', 'pickles', 'hummus', 'toasted']\n",
      "['wrap', 'falafel', 'greens', 'pickles', 'hummus', 'toasted']\n",
      "['wrap', 'falafel', 'greens', 'pickles', 'hummus', 'toasted']\n"
     ]
    }
   ],
   "source": [
    "# 串列推導移除標點符號 因stopwords變數有改1\n",
    "token_list3 = [[word for word in doc if word not in string.punctuation] for doc in token_list2]\n",
    "print(token_list3[3][1:7])\n",
    "\n",
    "# 串列推導移除所有數字(4 不見了)\n",
    "token_list4 = [[word for word in doc if not word.isdigit()] for doc in token_list3]\n",
    "print(token_list4[3][1:7])\n",
    "\n",
    "# 三層巢狀串列推導移除字符中夾雜數字或標點符號的情形\n",
    "token_list5 = [[''.join([i for i in word if not i.isdigit() and i not in string.punctuation]) for word in doc] for doc in token_list4]\n",
    "# doc: 各則簡訊，word: 各則簡訊中的各個字詞，i: 各個字詞中的各個字元\n",
    "# £10,000 變成£\n",
    "print(token_list5[3][1:7])\n",
    "\n",
    "# 串列推導移除空元素\n",
    "token_list6 = [list(filter(None, doc)) for doc in token_list5]\n",
    "print(token_list6[3][1:7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6d7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/gaoming/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ad8ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrap', 'falafel', 'green', 'pickle', 'hummus', 'toasted']\n",
      "['setting perfectly adequate food come close dining chain like chili victoria station barbecue betterit s surprise always pick coupon linwood restaurantcom', 'nothing special good enough like another one much better dorchester hardly get area le s goto place pho']\n"
     ]
    }
   ],
   "source": [
    "# 載入nltk.stem 的WordNet 詞形還原庫\n",
    "# 宣告詞形還原器\n",
    "lemma = WordNetLemmatizer()\n",
    "# 串列推導完成詞形還原(needs 變成need)\n",
    "token_list6 = [[lemma.lemmatize(word) for word in doc] for doc in token_list6]\n",
    "print(token_list6[3][1:7])\n",
    "\n",
    "# 串列推導完成各則字詞的串接\n",
    "# join() 方法將各則簡訊doc 中分開的字符又連接起來\n",
    "token_list7 = [' '.join(doc) for doc in token_list6]\n",
    "print(token_list7[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e335bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 5696)\t1\n",
      "  (0, 4494)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 1074)\t1\n",
      "  (0, 1591)\t1\n",
      "  (0, 913)\t1\n",
      "  (0, 1008)\t1\n",
      "  (0, 6064)\t1\n",
      "  (0, 386)\t1\n",
      "  (0, 6226)\t1\n",
      "  (0, 4516)\t1\n",
      "  (0, 1283)\t1\n",
      "  (0, 2188)\t1\n",
      "  (0, 1131)\t1\n",
      "  (1, 5948)\t1\n",
      "  (1, 1675)\t1\n",
      "  (1, 2862)\t1\n",
      "  (1, 207)\t1\n",
      "  (1, 3314)\t1\n",
      "  (1, 2661)\t1\n",
      "  (1, 4511)\t1\n",
      "  (1, 2560)\t1\n",
      "  (1, 2662)\t1\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(512221, 7202)\n",
      "7202\n",
      "['appropriate', 'appropriately', 'approximately', 'apps', 'april', 'aquarium', 'arancini', 'area', 'area definitely', 'area food', 'area great', 'area nt', 'area place', 'area s', 'argue', 'arm', 'aroma', 'arrangement', 'array', 'arrival', 'arrive', 'arrived', 'arrived minute', 'arrived pm', 'arrives', 'arriving', 'art', 'artichoke', 'artichoke dip', 'artist', 'artwork', 'arugula', 'arugula salad', 'asada', 'asap', 'asia', 'asian', 'asian food', 'asian fusion', 'asian restaurant', 'aside', 'ask', 'ask u', 'asked', 'asked server', 'asked u', 'asked waiter', 'asked waitress', 'asked wanted', 'asking', 'asks', 'asparagus', 'aspect', 'associate', 'assorted', 'assortment', 'assume', 'assumed', 'assuming', 'assured', 'ate', 'atlantic', 'atm', 'atmosphere', 'atmosphere food', 'atmosphere fun', 'atmosphere good', 'atmosphere great', 'atmosphere nice', 'atmosphere place', 'atmosphere really', 'atmosphere s', 'atmosphere service', 'atop', 'atrocious', 'attached', 'attack', 'attempt', 'attempted', 'attend', 'attended', 'attending', 'attention', 'attentive', 'attentive food', 'attentive friendly', 'attentive service', 'attitude', 'attraction', 'attractive', 'au', 'august', 'authentic', 'authentic chinese', 'authentic italian', 'authentic mexican', 'authenticity', 'automatically', 'available', 'ave', 'avenue', 'average', 'average best', 'average food', 'avocado']\n",
      "[[    27      0]\n",
      " [    34      0]\n",
      " [    69      0]\n",
      " ...\n",
      " [512198      0]\n",
      " [512206      0]\n",
      " [512220      0]]\n",
      "      avenue  average  average best  average food  avocado\n",
      "4460       0        0             0             0        0\n",
      "4461       0        0             0             0        0\n",
      "4462       0        0             0             0        0\n",
      "4463       0        0             0             0        0\n",
      "4464       0        0             0             0        0\n",
      "4465       0        0             0             0        0\n",
      "4466       0        0             0             0        0\n",
      "4467       0        0             0             0        0\n",
      "4468       0        0             0             0        0\n",
      "4469       0        0             0             0        0\n"
     ]
    }
   ],
   "source": [
    "# 從feature_extraction 模組載入詞頻計算與DTM 建構類別\n",
    "# 宣告空模\n",
    "# max_df: corpus-specific stop words (特定於語料庫的停用詞) 此處文件頻率超出六成(0.6)者不計入詞彙中;\n",
    "# min_df: this value is also called cut-off in the literature 此處表文件頻率低於20者不計入詞彙中 (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "# min_df: 類似於max_df，不同之處在於如果某個詞的document frequence小於min_df，則這個詞不會被當作關鍵詞\n",
    "# ngram_range:片語切分的長度範圍，待詳解\n",
    "# stop_words\t設定停用詞，設為english將使用內建的英語停用詞，設為一個list可自定義停用詞，設為None不使用停用詞，設為None且max_df∈[0.7, 1.0)將自動根據當前的語料庫建立停用詞表\n",
    "#\n",
    "vec = CountVectorizer(min_df=500, max_df=0.1, ngram_range=(1, 2), token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", stop_words='english')\n",
    "# 傳入簡訊配適實模並轉換為DTM 稀疏矩陣X\n",
    "X = vec.fit_transform(token_list7)\n",
    "# scipy 套件稀疏矩陣類別\n",
    "\n",
    "print(type(X))\n",
    "\n",
    "# 稀疏矩陣儲存詞頻的方式：(橫列，縱行) 詞頻\n",
    "print(X[:2])  # 前兩則簡訊的詞頻在稀疏矩陣中的存放方式\n",
    "\n",
    "#import sys\n",
    "#import numpy\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(X.toarray()[:2])  # 轉成常規矩陣後，方可見前兩則簡訊的完整詞頻向量\n",
    "\n",
    "# X 轉為常規矩陣(X.toarray())，並組織為pandas 資料框\n",
    "sms_dtm = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "# 512221 列(則)7202 行(字) 的結構\n",
    "print(sms_dtm.shape)\n",
    "\n",
    "# 模型vec 取出DTM 各字詞的get_feature_names() 方法\n",
    "print(len(vec.get_feature_names()))  # 共有7202 個字詞\n",
    "\n",
    "print(vec.get_feature_names()[200:305])\n",
    "\n",
    "\n",
    "# 512221 則中app 此字只有 則正詞頻，的確稀疏(新版numpy請用下行註解程式碼)\n",
    "#print(np.argwhere(sms_dtm['awesome'] > 0))  # 列向量\n",
    "print(np.argwhere((sms_dtm['awesome'] > 0).values.reshape((-1,1)))) # 新版numpy需轉成行向量\n",
    "\n",
    "# DTM 部分內容\n",
    "print(sms_dtm.iloc[4460:4470, 300:305])\n",
    "# sms_dtm.max().max() # 15, 原始詞頻dtm，適合配適multinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e760a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.641686\n",
      "0    0.358314\n",
      "Name: type, dtype: float64\n",
      "1    0.637527\n",
      "0    0.362473\n",
      "Name: type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 訓練與測試集切分(sms_raw, sms_dtm, token_list6)\n",
    "sms_raw_train = sms_raw.iloc[:417000, :]\n",
    "sms_raw_test = sms_raw.iloc[417000:, :]\n",
    "sms_dtm_train = sms_dtm.iloc[:417000, :]\n",
    "sms_dtm_test = sms_dtm.iloc[417000:, :]\n",
    "token_list6_train = token_list6[:417000]\n",
    "token_list6_test = token_list6[417000:]\n",
    "# 查核各子集類別分佈\n",
    "print(sms_raw_train['type'].value_counts()/len(sms_raw_train['type']))\n",
    "\n",
    "print(sms_raw_test['type'].value_counts()/len(sms_raw_test['type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef6dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud() 統計詞頻須跨篇組合所有詞項\n",
    "#tokens_train = [token for doc in token_list6_train for token in doc]\n",
    "#print(len(tokens_train))\n",
    "\n",
    "# 邏輯值索引結合zip() 綑綁函數，再加判斷句與串列推導\n",
    "#tokens_train_spam = [token for is_spam, doc in zip(sms_raw_train['type'] == '0', token_list6_train) if is_spam for token in doc]\n",
    "# 取出正常簡訊\n",
    "#tokens_train_ham = [token for is_ham, doc in zip(sms_raw_train['type'] == '1', token_list6_train) if is_ham for token in doc]\n",
    "# 逗號接合訓練與spam 和ham 兩子集tokens\n",
    "#str_train = ','.join(tokens_train)\n",
    "#str_train_spam = ','.join(tokens_train_spam)\n",
    "#str_train_ham = ','.join(tokens_train_ham)\n",
    "# Python 文字雲套件(conda install -c conda-forge wordcloud --y, !conda install -c conda-forge wordcloud --y)\n",
    "# 宣告文字雲物件(最大字數max_words 預設為200)\n",
    "#wc_train = WordCloud(background_color=\"white\", prefer_horizontal=0.5)\n",
    "# 傳入資料統計，並產製文字雲物件\n",
    "#wc_train.generate(str_train_spam)  # str_train -> str_train_ham, str_train_spam\n",
    "# 呼叫matplotlib.pyplot 模組下的imshow() 方法繪圖\n",
    "#plt.imshow(wc_train)\n",
    "#plt.axis(\"off\")\n",
    "# plt.show()\n",
    "# plt.savefig('wc_train.png')\n",
    "# 限於篇幅，str_train_spam 和str_train_ham 文字雲繪製代碼省略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6cc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 訓練集正確率為0.8489568345323741\n",
      " 測試集正確率為0.8472395795045211\n",
      "[149417. 267583.]\n",
      "[[355. 256. 299. ... 673. 231. 314.]\n",
      " [501. 388. 387. ...  14. 417. 790.]]\n",
      "(2, 7202)\n",
      "[[ -9.91215884 -10.23801348 -10.08330709  -7.29891333]\n",
      " [ -9.95995355 -10.21497433 -10.21754833  -7.0856464 ]]\n",
      "(2, 7202)\n",
      "(2, 7202)\n",
      "[[4.95683101e-05 3.57838643e-05 4.17710479e-05 6.76273265e-04]\n",
      " [4.72549309e-05 3.66178648e-05 3.65237315e-05 8.37033558e-04]]\n",
      "[1. 1.]\n",
      "[0.00413185 0.00361867]\n",
      "[['minute' 'said' 'bad' 'asked' 'star' 'way' 'dish' 'think' 'know' 'say']\n",
      " ['dish' 'sauce' 'fresh' 'dinner' 'meal' 'recommend' 'favorite' 'day'\n",
      "  'right' 'beer']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# 載入多項式天真貝氏模型類別\n",
    "# 模型定義、配適與預測\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(sms_dtm_train, sms_raw_train['type'])\n",
    "train = clf.predict(sms_dtm_train)\n",
    "print(\" 訓練集正確率為{}\".format(sum(sms_raw_train['type'] == train)/len(train)))\n",
    "\n",
    "pred = clf.predict(sms_dtm_test)\n",
    "print(\" 測試集正確率為{}\".format(sum(sms_raw_test['type'] == pred)/len(pred)))\n",
    "\n",
    "# 訓練所用的各類樣本數\n",
    "print(clf.class_count_)\n",
    "\n",
    "# 兩類與7612(7484) 個屬性的交叉列表\n",
    "print(clf.feature_count_)\n",
    "\n",
    "print(clf.feature_count_.shape)\n",
    "\n",
    "# 已知類別下，各屬性之條件機率Pr[x_i|y] 的對數值\n",
    "print(clf.feature_log_prob_[:, :4])\n",
    "\n",
    "print(clf.feature_log_prob_.shape)\n",
    "\n",
    "# 將對數條件機率轉成機率值(補充程式碼)\n",
    "feature_prob = np.exp(clf.feature_log_prob_)\n",
    "print(feature_prob.shape)\n",
    "print(feature_prob[:, :4])\n",
    "# 驗證兩類之機率值總和為1(補充程式碼)\n",
    "print(np.apply_along_axis(np.sum, 1, feature_prob))  # [1. 1.]\n",
    "# 兩類最大字詞機率值(補充程式碼)\n",
    "print(np.apply_along_axis(np.max, 1, feature_prob))  # [0.00813987 0.01839848]]\n",
    "# 抓出兩類機率前十高的字詞，與文字雲結果？？？(補充程式碼)\n",
    "print(sms_dtm.columns.values[np.argsort(-feature_prob)[:, :10]])\n",
    "# ham: [['minute' 'said' 'bad' 'asked' 'star' 'way' 'dish' 'think' 'know' 'say']\n",
    "# spam: ['dish' 'sauce' 'fresh' 'dinner' 'meal' 'recommend' 'favorite' 'day' 'right' 'beer']]\n",
    "# ------------------------------------------------------------------------------\n",
    "# 載入sklearn 交叉驗證模型選擇的重要函數\n",
    "# 自定義k 摺交叉驗證模型績效計算函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab9e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want' 'going' 'meal' 'sauce' 'told' 'waitress' 'customer' 'day' 'took'\n",
      "  'dinner']\n",
      " ['small' 'bit' 'cheese' 'sandwich' 'lunch' 'area' 'lot' 'flavor' 'pizza'\n",
      "  'perfect']]\n"
     ]
    }
   ],
   "source": [
    "# 抓出兩類機率10～20的字詞\n",
    "print(sms_dtm.columns.values[np.argsort(-feature_prob)[:, 10:20]])\n",
    "#[['want' 'going' 'meal' 'sauce' 'told' 'waitress' 'customer' 'day' 'took' 'dinner']\n",
    "# ['small' 'bit' 'cheese' 'sandwich' 'lunch' 'area' 'lot' 'flavor' 'pizza' 'perfect']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69f892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5摺交叉驗證結果如下：\n",
      "[0.84438479 0.84542775 0.84719456 0.84763383 0.84817071]\n",
      " 平均正確率：0.847(+/-標準誤0.001)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # 創建k 摺交叉驗證迭代器(iterator)，用於X 與y 的切分\n",
    "    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print(\"{}摺交叉驗證結果如下：\\n{}\".format(K, scores))\n",
    "    tmp = \" 平均正確率：{0:.3f}(+/-標準誤{1:.3f})\"\n",
    "    print(tmp.format(np.mean(scores), sem(scores)))\n",
    "\n",
    "\n",
    "evaluate_cross_validation(clf, sms_dtm, sms_raw['type'], 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
